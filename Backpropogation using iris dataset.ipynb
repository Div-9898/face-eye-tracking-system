{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb85ea2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# # Manual Backpropagation Implementation for Iris Dataset\n",
    "# \n",
    "# This notebook implements a complete neural network with manual backpropagation from scratch.\n",
    "# We'll build everything step-by-step without using any automatic differentiation libraries.\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Iris Dataset...\")\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: 0=setosa, 1=versicolor, 2=virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[y]\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Classes: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a37900",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Data Preprocessing\n",
    "# \n",
    "# We'll standardize the features and one-hot encode the target variable.\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the target variable\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"Convert integer labels to one-hot encoded vectors\"\"\"\n",
    "    encoded = np.zeros((len(y), num_classes))\n",
    "    for i, val in enumerate(y):\n",
    "        encoded[i, val] = 1\n",
    "    return encoded\n",
    "\n",
    "y_encoded = one_hot_encode(y, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Feature shape: {X_train.shape[1]}\")\n",
    "print(f\"Output classes: {y_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d279f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Manual Neural Network Implementation\n",
    "# \n",
    "# Now we'll implement our neural network class with manual backpropagation.\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize neural network with random weights and zero biases\n",
    "        Architecture: Input -> Hidden -> Output\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        # Xavier/Glorot initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store activations and derivatives for backpropagation\n",
    "        self.z1 = None  # Pre-activation (hidden layer)\n",
    "        self.a1 = None  # Post-activation (hidden layer)\n",
    "        self.z2 = None  # Pre-activation (output layer)\n",
    "        self.a2 = None  # Post-activation (output layer)\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da229130",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Activation Functions\n",
    "# \n",
    "# Let's implement our activation functions and their derivatives:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ad845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function for output layer\"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa25b34",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Forward Pass Implementation\n",
    "# \n",
    "# The forward pass computes predictions by passing data through the network:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Linear transformation\n",
    "        self.a1 = self.sigmoid(self.z1)         # Activation\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Linear transformation\n",
    "        self.a2 = self.softmax(self.z2)               # Softmax activation\n",
    "        \n",
    "        return self.a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9810617",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Loss Function\n",
    "# \n",
    "# We'll use cross-entropy loss for multi-class classification:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f180a0c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Manual Backpropagation Implementation\n",
    "# \n",
    "# This is the core of our implementation - computing gradients manually using the chain rule:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Manual backpropagation implementation\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Step 1: Compute output layer gradients\n",
    "        # dL/dz2 = a2 - y_true (derivative of softmax + cross-entropy)\n",
    "        dz2 = self.a2 - y_true\n",
    "        \n",
    "        # dL/dW2 = a1^T * dz2\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        \n",
    "        # dL/db2 = mean(dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Step 2: Compute hidden layer gradients\n",
    "        # dL/da1 = dz2 * W2^T\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        \n",
    "        # dL/dz1 = da1 * sigmoid'(z1)\n",
    "        dz1 = da1 * self.sigmoid_derivative(self.z1)\n",
    "        \n",
    "        # dL/dW1 = X^T * dz1\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        \n",
    "        # dL/db1 = mean(dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756ed3e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Parameter Update and Training Methods\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5fd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "        \"\"\"\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Single training step: forward pass + backward pass + parameter update\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        predictions = self.forward_pass(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(y, predictions)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2 = self.backward_pass(X, y)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters(dW1, db1, dW2, db2)\n",
    "        \n",
    "        return loss, predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.forward_pass(X)\n",
    "    \n",
    "    def calculate_accuracy(self, X, y_true):\n",
    "        \"\"\"Calculate accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "        return accuracy\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, print_every=100):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \"\"\"\n",
    "        print(f\"Training neural network for {epochs} epochs...\")\n",
    "        print(f\"Architecture: {self.input_size} -> {self.hidden_size} -> {self.output_size}\")\n",
    "        print(f\"Learning rate: {self.learning_rate}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training step\n",
    "            loss, _ = self.train_step(X_train, y_train)\n",
    "            \n",
    "            # Calculate accuracies\n",
    "            train_acc = self.calculate_accuracy(X_train, y_train)\n",
    "            val_acc = self.calculate_accuracy(X_val, y_val)\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(loss)\n",
    "            self.accuracy_history.append(train_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % print_every == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | \"\n",
    "                      f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502e040",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Train the Neural Network\n",
    "# \n",
    "# Now let's create and train our neural network:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f71b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the neural network\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MANUAL BACKPROPAGATION NEURAL NETWORK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create neural network\n",
    "nn = ManualNeuralNetwork(\n",
    "    input_size=4,      # 4 features in Iris dataset\n",
    "    hidden_size=8,     # 8 hidden neurons\n",
    "    output_size=3,     # 3 classes\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "nn.train(X_train, y_train, X_test, y_test, epochs=1000, print_every=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ccc98",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Evaluate the Model\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_accuracy = nn.calculate_accuracy(X_train, y_train)\n",
    "test_accuracy = nn.calculate_accuracy(X_test, y_test)\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = nn.predict(X_test)\n",
    "predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    pred_class = iris.target_names[predicted_classes[i]]\n",
    "    true_class = iris.target_names[true_classes[i]]\n",
    "    confidence = test_predictions[i][predicted_classes[i]]\n",
    "    print(f\"Sample {i+1}: Predicted={pred_class}, True={true_class}, Confidence={confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c859e8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Visualize Training Progress\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn.loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn.accuracy_history)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0bfd6",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Detailed Backpropagation Analysis\n",
    "# \n",
    "# Let's examine the internals of our backpropagation implementation:\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554257d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the backpropagation process\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BACKPROPAGATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Network Architecture:\")\n",
    "print(f\"Input Layer: {nn.input_size} neurons (features)\")\n",
    "print(f\"Hidden Layer: {nn.hidden_size} neurons with sigmoid activation\")\n",
    "print(f\"Output Layer: {nn.output_size} neurons with softmax activation\")\n",
    "\n",
    "print(f\"\\nWeight Matrices:\")\n",
    "print(f\"W1 shape: {nn.W1.shape} (input to hidden)\")\n",
    "print(f\"W2 shape: {nn.W2.shape} (hidden to output)\")\n",
    "\n",
    "print(f\"\\nBias Vectors:\")\n",
    "print(f\"b1 shape: {nn.b1.shape} (hidden layer bias)\")\n",
    "print(f\"b2 shape: {nn.b2.shape} (output layer bias)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233fc6a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ### Step-by-Step Forward and Backward Pass Demo\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate one forward and backward pass\n",
    "print(f\"\\nDemonstrating one forward-backward pass with first training sample:\")\n",
    "sample_X = X_train[:1]  # First sample\n",
    "sample_y = y_train[:1]  # First sample target\n",
    "\n",
    "print(f\"Input: {sample_X}\")\n",
    "print(f\"Target: {sample_y}\")\n",
    "\n",
    "# Forward pass\n",
    "output = nn.forward_pass(sample_X)\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Predicted class: {iris.target_names[np.argmax(output)]}\")\n",
    "\n",
    "# Show intermediate activations\n",
    "print(f\"\\nIntermediate activations:\")\n",
    "print(f\"z1 (hidden pre-activation): {nn.z1}\")\n",
    "print(f\"a1 (hidden post-activation): {nn.a1}\")\n",
    "print(f\"z2 (output pre-activation): {nn.z2}\")\n",
    "print(f\"a2 (output post-activation): {nn.a2}\")\n",
    "\n",
    "# Backward pass\n",
    "dW1, db1, dW2, db2 = nn.backward_pass(sample_X, sample_y)\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"dW1 shape: {dW1.shape}, mean absolute value: {np.mean(np.abs(dW1)):.6f}\")\n",
    "print(f\"db1 shape: {db1.shape}, mean absolute value: {np.mean(np.abs(db1)):.6f}\")\n",
    "print(f\"dW2 shape: {dW2.shape}, mean absolute value: {np.mean(np.abs(dW2)):.6f}\")\n",
    "print(f\"db2 shape: {db2.shape}, mean absolute value: {np.mean(np.abs(db2)):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6de26",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Summary\n",
    "# \n",
    "# This implementation demonstrates complete manual backpropagation including:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e63999",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"This implementation includes:\")\n",
    "print(\"1. Manual forward propagation\")\n",
    "print(\"2. Manual backpropagation with gradient calculations\")\n",
    "print(\"3. Manual parameter updates using gradient descent\")\n",
    "print(\"4. No use of automatic differentiation libraries\")\n",
    "print(\"5. Complete training loop with loss and accuracy tracking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
